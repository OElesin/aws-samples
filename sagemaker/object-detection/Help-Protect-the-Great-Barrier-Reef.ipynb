{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ba734f",
   "metadata": {},
   "source": [
    "<center><h1>Help Protect the Great Barrier Reef with Amazon SageMaker Objection Detection</h1></center>\n",
    "\n",
    "![Chest X-Ray Images (Pneumonia)](https://storage.googleapis.com/kaggle-competitions/kaggle/31703/logos/header.png?t=2021-10-29-00-30-04)\n",
    "    \n",
    "Data Source: https://www.kaggle.com/c/tensorflow-great-barrier-reef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7508b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle imagesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ead778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "!kaggle competitions download -c tensorflow-great-barrier-reef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8178b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imagesize\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aac0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import glob\n",
    "\n",
    "import shutil\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b79618",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'dataset/great-barrier-reef'\n",
    "\n",
    "training_image = retrieve('object-detection', boto3.Session().region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip tensorflow-great-barrier-reef.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a42fbf",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "1. Filter out images without annotations\n",
    "2. Create directories for images and annotations\n",
    "3. Generate annotation files having the same name with the corresponding image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e06c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 2 # which fold to train\n",
    "REMOVE_NOBBOX = True # remove images with no bbox\n",
    "ROOT_DIR  = 'tensorflow-great-barrier-reef'\n",
    "IMAGE_DIR = 'images' # directory to save images\n",
    "LABEL_DIR = 'annotation' # directory to save labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a0c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {IMAGE_DIR}\n",
    "!mkdir -p {LABEL_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(row):\n",
    "    row['old_image_path'] = f'train_images/video_{row.video_id}/{row.video_frame}.jpg'\n",
    "    row['image_path'] = f'{IMAGE_DIR}/video_{row.video_id}_{row.video_frame}.jpg'\n",
    "    row['label_path'] = f'{LABEL_DIR}/video_{row.video_id}_{row.video_frame}.json'\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d216017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data\n",
    "df = pd.read_csv('train.csv')\n",
    "df = df.apply(get_path, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710da0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['annotations'] = df['annotations'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400e1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_bbox'] = df['annotations'].apply(lambda x: len(x))\n",
    "data = (df.num_bbox>0).value_counts()/len(df)*100\n",
    "print(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78fc6b",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb53850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if REMOVE_NOBBOX:\n",
    "    df = df.query(\"num_bbox>0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a68a9",
   "metadata": {},
   "source": [
    "We need to generate the content of the annotation file following the example below. This makes the \n",
    "\n",
    "```json\n",
    "{\n",
    "   \"file\": \"your_image_directory/sample_image1.jpg\",\n",
    "   \"image_size\": [\n",
    "      {\n",
    "         \"width\": 50,\n",
    "         \"height\": 32,\n",
    "         \"depth\": 3\n",
    "      }\n",
    "   ],\n",
    "   \"annotations\": [\n",
    "      {\n",
    "         \"class_id\": 0,\n",
    "         \"left\": 559,\n",
    "         \"top\": 213,\n",
    "         \"width\": 50,\n",
    "         \"height\": 32\n",
    "      }\n",
    "   ],\n",
    "   \"categories\": [\n",
    "      {\n",
    "         \"class_id\": 0,\n",
    "         \"name\": \"starfish\"\n",
    "      }\n",
    "   ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c48cfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sm_annotation(row):\n",
    "    \"\"\"\n",
    "    Function to generate SageMaker Object Detection\n",
    "    Annotation json file.\n",
    "    \"\"\"\n",
    "    annotation = row.annotations\n",
    "    old_image_path = row.old_image_path\n",
    "    im = Image.open(old_image_path)\n",
    "    width, height = im.size\n",
    "    # get image size\n",
    "    annotation_object = {\n",
    "        'file': row.image_path,\n",
    "        'image_size': [\n",
    "            {'width': width, 'height': height, 'depth': 3}\n",
    "        ],\n",
    "        'categories': [\n",
    "            {'class_id': 0, 'name': 'starfish'}\n",
    "        ]\n",
    "    }\n",
    "    annotation_object['annotations'] = [ \n",
    "        {'class_id': 0, 'width': i['width'], 'height': i['height'], 'left': i['x'], 'top': i['y']}\n",
    "        for i in row.annotations\n",
    "    ]\n",
    "    return annotation_object\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sm_annotations'] = df.apply(generate_sm_annotation, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb4cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = train_test_split(df, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf7650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ed122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_copy(path, class_):\n",
    "    data = path.split('/')\n",
    "    filename = data[-1]\n",
    "    video_id = data[-2]\n",
    "    new_path = os.path.join(f'images/{class_}',f'{video_id}_{filename}')\n",
    "    shutil.copy(path, new_path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cac4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sm_annotations(annotation, class_):\n",
    "    \"\"\"\n",
    "    Function to write SM Object Detection annotation \n",
    "    to file\n",
    "    \"\"\"\n",
    "    data = annotation['file'].split('/')\n",
    "    top_path = data[0]\n",
    "    video_id = data[1]\n",
    "    new_path = f'{top_path}/{class_}_annotation/{video_id}'.replace('jpg', 'json')\n",
    "    annotation['file'] = f'{video_id}'\n",
    "    annotation_output_file = open(new_path, 'w')\n",
    "    json.dump(annotation, annotation_output_file, indent=0)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105607ce",
   "metadata": {},
   "source": [
    "### Copy train images and annotation files to the respective paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da6e79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_paths = train.old_image_path.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc2a30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = Parallel(n_jobs=-1, backend='threading')(delayed(make_copy)(path, 'train') for path in tqdm(train_image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a06f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_sm_annotations = train.sm_annotations.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef0e541",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_sm_annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ccda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = Parallel(n_jobs=-1, backend='threading')(delayed(write_sm_annotations)(annotation, 'train') for annotation in tqdm(train_image_sm_annotations))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dacf90a",
   "metadata": {},
   "source": [
    "### Copy validation images and annotation files to the respective paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e19fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_image_paths = validation.old_image_path.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbefb10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = Parallel(n_jobs=-1, backend='threading')(delayed(make_copy)(path, 'validation') for path in tqdm(validation_image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36531335",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_image_sm_annotations = validation.sm_annotations.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d2c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_image_sm_annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cbf0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = Parallel(n_jobs=-1, backend='threading')(delayed(write_sm_annotations)(annotation, 'validation') for annotation in tqdm(validation_image_sm_annotations))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4970ccca",
   "metadata": {},
   "source": [
    "Let's inspect some images for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8879e71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bdc6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'images/validation/video_0_11923.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855baa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(image_path)\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(im)\n",
    "\n",
    "# Create a Rectangle patch\n",
    "rect = patches.Rectangle((554, 360), 42, 34, linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "# Add the patch to the Axes\n",
    "ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34451043",
   "metadata": {},
   "source": [
    "### Upload data to S3 for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Four channels: train, validation, train_lst, and validation_lst\n",
    "s3_train = 's3://{}/{}/train/'.format(bucket, prefix)\n",
    "s3_validation = 's3://{}/{}/validation/'.format(bucket, prefix)\n",
    "s3_train_annotation = 's3://{}/{}/train_annotation/'.format(bucket, prefix)\n",
    "s3_validation_annotation = 's3://{}/{}/validation_annotation/'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5787fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237312c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp images/train/ $s3_train --recursive --dryrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384959f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the image files to train and validation channels\n",
    "!aws s3 cp images/train/ $s3_train --recursive --quiet\n",
    "!aws s3 cp images/validation/ $s3_validation --recursive --quiet\n",
    "\n",
    "!aws s3 cp images/train_annotation/ $s3_train_annotation --recursive --quiet\n",
    "!aws s3 cp images/validation_annotation/ $s3_validation_annotation --recursive --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5467bf",
   "metadata": {},
   "source": [
    "### Fine-tuning the Object Classification Model\n",
    "\n",
    "Once we have the data available in the correct format for training, the next step is to actually train the model using the data. Before training the model, we need to setup the training parameters. The next section will explain the parameters in detail.\n",
    "\n",
    "\n",
    "#### Training parameters\n",
    "There are two kinds of parameters that need to be set for training. The first one are the parameters for the training job. These include:\n",
    "\n",
    "- Training instance count: This is the number of instances on which to run the training. When the number of instances is greater than one, then the image classification algorithm will run in distributed settings.\n",
    "- Training instance type: This indicates the type of machine on which to run the training. Typically, we use GPU instances for these training\n",
    "- Output path: This the s3 folder in which the training output is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a49549",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use_spot_instances = True\n",
    "train_max_run=1300\n",
    "train_max_wait = 2400 if train_use_spot_instances else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865dd6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "\n",
    "object_detector_model = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role, \n",
    "    instance_count=1, \n",
    "    instance_type='ml.p2.xlarge',\n",
    "    volume_size = 50,\n",
    "    input_mode= 'File',\n",
    "    use_spot_instances=train_use_spot_instances,\n",
    "    max_run=train_max_run,\n",
    "    max_wait=train_max_run,    \n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd96ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 1\n",
    "num_training_samples = train.shape[0]\n",
    "print(\"num classes: {}, num training images: {}\".format(num_classes, num_training_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3682b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "lr_steps = \"33,67\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751287e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detector_model.set_hyperparameters(\n",
    "    base_network=\"resnet-50\",\n",
    "    use_pretrained_model=1,\n",
    "    num_classes=num_classes,\n",
    "    mini_batch_size=16,\n",
    "    epochs=num_epochs,\n",
    "    learning_rate=0.001,\n",
    "    lr_scheduler_step=lr_steps,\n",
    "    lr_scheduler_factor=0.1,\n",
    "    optimizer=\"sgd\",\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005,\n",
    "    overlap_threshold=0.5,\n",
    "    nms_threshold=0.45,\n",
    "    image_shape=512,\n",
    "    label_width=350,\n",
    "    num_training_samples=num_training_samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_train, distribution='FullyReplicated', \n",
    "    content_type='application/x-image', s3_data_type='S3Prefix'\n",
    ")\n",
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_validation, distribution='FullyReplicated',\n",
    "    content_type='application/x-image', s3_data_type='S3Prefix'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45126635",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_annotation = sagemaker.inputs.TrainingInput(\n",
    "    s3_train_annotation, distribution='FullyReplicated', \n",
    "    content_type='application/x-image', s3_data_type='S3Prefix'\n",
    ")\n",
    "validation_data_annotation = sagemaker.inputs.TrainingInput(\n",
    "    s3_validation_annotation, distribution='FullyReplicated', \n",
    "    content_type='application/x-image', s3_data_type='S3Prefix'\n",
    ")\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data, \n",
    "                 'train_annotation': train_data_annotation, 'validation_annotation': validation_data_annotation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9645682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19914f90",
   "metadata": {},
   "source": [
    "## Submit training job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d50376",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "object_detector_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595368a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_latest_p37",
   "language": "python",
   "name": "conda_mxnet_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
