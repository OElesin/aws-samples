{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Supporting RSNA Screening Mammography Breast Cancer Detection with Pytorch Image Classification on Amazon SageMaker</h1></center>\n",
    "\n",
    "![Find breast cancers in screening mammograms](https://storage.googleapis.com/kaggle-competitions/kaggle/39272/logos/header.png?t=2022-11-28-17-29-35)\n",
    "    \n",
    "Data Source: https://www.kaggle.com/competitions/rsna-breast-cancer-detection/data?select=train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker ipywidgets pydicom --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U pylibjpeg pylibjpeg-openjpeg pylibjpeg-libjpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install split-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kaggle\n",
    "# !kaggle competitions download -c rsna-breast-cancer-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip chest-xray-pneumonia.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import re\n",
    "import os, sys, glob\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'rsna-breast-cancer-detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_role = get_execution_role()\n",
    "aws_region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pydicom\n",
    "from pydicom.data import get_testdata_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "import json\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Quick Data inspection\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pydicom.dcmread('train_images/10006/1864590858.dcm')\n",
    "plt.imshow(ds.pixel_array, cmap=plt.cm.bone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Data and Metadata Exploration\n",
    "\n",
    "#### Metadata file column desriptions\n",
    "\n",
    "- `site_id` - ID code for the source hospital.\n",
    "- `patient_id` - ID code for the patient.\n",
    "- `image_id` - ID code for the image.\n",
    "- `laterality` - Whether the image is of the left or right breast.\n",
    "- `view` - The orientation of the image. The default for a screening exam is to capture two views per breast.\n",
    "- `age` - The patient's age in years.\n",
    "- `implant` - Whether or not the patient had breast implants. Site 1 only provides breast implant information at the patient level, not at the breast level.\n",
    "- `density` - A rating for how dense the breast tissue is, with A being the least dense and D being the most dense. Extremely dense tissue can make diagnosis more difficult. Only provided for train.\n",
    "- `machine_id` - An ID code for the imaging device.\n",
    "- `cancer` - Whether or not the breast was positive for malignant cancer. The target value. Only provided for train.\n",
    "- `biopsy` - Whether or not a follow-up biopsy was performed on the breast. Only provided for train.\n",
    "- `invasive` - If the breast is positive for cancer, whether or not the cancer proved to be invasive. Only provided for train.\n",
    "- `BIRADS` - 0 if the breast required follow-up, 1 if the breast was rated as negative for cancer, and 2 if the breast was rated as normal. Only provided for train.\n",
    "- `difficult_negative_case` - True if the case was unusually difficult. Only provided for train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_df.density.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Preparation \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Dataset Imbalance\n",
    "\n",
    "On exploring the data with the `train.csv`, we can easily see that only 2% of the data has positive cancer labels. Hence, if we train our model on the data, we may likely end up with a model predicting a lot of false negatives. Therefore to mitigate this, we prepare a balanced data set across the following features:\n",
    "1. density (optional)\n",
    "2. biopsy\n",
    "3. invasive\n",
    "4. BIRADS\n",
    "\n",
    "Reason: The problem is originally a binary classification problem, i.e., 0 or 1. We want to use leverage the ML model capability to detect the need for biopsy, the invasive nature of the cancer and follow-up is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_df.cancer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_df.BIRADS.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balancing_columns = ['density', 'invasive', 'biopsy', 'BIRADS', 'cancer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_metadata_df\n",
    "df_grouped_by = train_metadata_df.groupby(balancing_columns)\n",
    "df_balanced = df_grouped_by.apply(lambda x: x.sample(df_grouped_by.size().min()).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = df_balanced.droplevel(balancing_columns)\n",
    "# df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/apache/incubator-mxnet/master/tools/im2rec.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert dcm files to png files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation\n",
    "\n",
    "The dataset is split into 2 classes Pneumonia and Normal. However, the Pneumonia directory contains images for both Bacteria and Virus Pneumonia. We will create 3 class problem by splitting the Pneumonia directory into Bacteria and Virus, thus helping us in ur medical diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced['file_path'] = 'train_images/' + df_balanced['patient_id'].astype(str) + '/' + df_balanced['image_id'].astype(str) + '.dcm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.density.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_images/train model_images/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in balancing_columns:\n",
    "    !mkdir model_images/train/$column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balancing_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dcm_to_png_with_path(dcm_file_path):\n",
    "    \"\"\"\n",
    "    Function to convert a DCM file to a PNG image and generate the corresponding output file path\n",
    "    based on the metadata in the DCM file.\n",
    "    \"\"\"\n",
    "    # Load the DCM file using pydicom\n",
    "    dcm_data = pydicom.dcmread(dcm_file_path)\n",
    "\n",
    "    # Get the pixel data from the DCM file as a numpy array\n",
    "    pixel_data = dcm_data.pixel_array\n",
    "\n",
    "    # Rescale the pixel data to 0-255 and convert it to uint8 data type\n",
    "    pixel_data = ((pixel_data - np.min(pixel_data)) / np.ptp(pixel_data) * 255.0).astype(np.uint8)\n",
    "\n",
    "    # Resize the image to 1024x1024 using PIL\n",
    "    image = Image.fromarray(pixel_data).resize((1024, 1024))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_png_path(s):\n",
    "    \"\"\"\n",
    "    Function to store data in converted images into the class path\n",
    "    \"\"\"\n",
    "    png_paths = []\n",
    "    if s['biopsy'] == 1:\n",
    "        png_paths.append('model_images/train/biopsy/{}.png'.format(s['image_id']))\n",
    "    if s['cancer'] == 1:\n",
    "        png_paths.append('model_images/train/cancer/{}.png'.format(s['image_id']))\n",
    "    if s['invasive'] == 1:\n",
    "        png_paths.append('model_images/train/invasive/{}.png'.format(s['image_id'])) \n",
    "    if s['density'] == 'A':\n",
    "        png_paths.append('model_images/train/density_A/{}.png'.format(s['image_id']))\n",
    "    if s['density'] == 'B':\n",
    "        png_paths.append('model_images/train/density_B/{}.png'.format(s['image_id']))\n",
    "    if s['density'] == 'C':\n",
    "        png_paths.append('model_images/train/density_C/{}.png'.format(s['image_id']))\n",
    "    if s['density'] == 'D':\n",
    "        png_paths.append('model_images/train/density_D/{}.png'.format(s['image_id']))\n",
    "    \n",
    "    # convert dcm file to png\n",
    "    image = convert_dcm_to_png_with_path(s['file_path'])\n",
    "    \n",
    "    # save to new png paths\n",
    "    for png_path in png_paths:\n",
    "        os.makedirs(os.path.dirname(png_path), exist_ok=True)\n",
    "        image.save(png_path)\n",
    "    return png_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate_png_path()\n",
    "df_balanced.apply(generate_png_path, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_dcm_to_png_with_path('train_images/52566/202476234.dcm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split images into train and upload images to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitfolders.ratio(\"model_images/train/\", output=\"model_images/upload_to_s3/\",\n",
    "    seed=1337, ratio=(.8, .2), group_prefix=None, move=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation (Coming Soon)\n",
    "\n",
    "After rebalancing our dataset, we reduced the sample size from 54706 to 288. A good ML model may be extremely difficult to come by with this size. Hence, we employ image data augmentation techniques to increase the size of our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the image files to train and validation channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`restart from here`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune Tensorflow pre-trained model on our custom breast cancer dataset\n",
    "\n",
    "Once we have the data available in the correct format for training, the next step is to actually train the model using the data. Before training the model, we need to setup the training parameters. The next section will explain the parameters in detail.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve JumpStart Training artifacts\n",
    "---\n",
    "Here, for the selected model, we retrieve the training docker container, the training algorithm source, the pre-trained base model, and a python dictionary of the training hyper-parameters that the algorithm accepts with their default values. Note that the model_version=\"*\" fetches the lates model. Also, we do need to specify the training_instance_type to fetch train_image_uri.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id, model_version, = (\n",
    "    \"pytorch-ic-mobilenet-v2\",\n",
    "    \"*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "from ipywidgets import Dropdown\n",
    "\n",
    "# download JumpStart model_manifest file.\n",
    "boto3.client(\"s3\").download_file(\n",
    "    f\"jumpstart-cache-prod-{aws_region}\", \"models_manifest.json\", \"models_manifest.json\"\n",
    ")\n",
    "with open(\"models_manifest.json\", \"rb\") as json_file:\n",
    "    model_list = json.load(json_file)\n",
    "\n",
    "# filter-out all the Image Classification models from the manifest list.\n",
    "ic_models_all_versions, ic_models = [\n",
    "    model[\"model_id\"] for model in model_list if \"-ic-\" in model[\"model_id\"]\n",
    "], []\n",
    "[ic_models.append(model) for model in ic_models_all_versions if model not in ic_models]\n",
    "\n",
    "# display the model-ids in a dropdown, for user to select a model.\n",
    "dropdown = Dropdown(\n",
    "    options=ic_models,\n",
    "    value=model_id,\n",
    "    description=\"JumpStart Image Classification Models:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "display(IPython.display.Markdown(\"## Select a JumpStart pre-trained model from the dropdown below\"))\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "\n",
    "model_id, model_version = dropdown.value, \"*\"\n",
    "training_instance_type = \"ml.p3.2xlarge\"\n",
    "\n",
    "# Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    image_scope=\"training\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "# Retrieve the training script\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"training\"\n",
    ")\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Training parameters\n",
    "Now that we are done with all the setup that is needed, we are ready to fine-tune our Image Classification model. To begin, let us create a sageMaker.estimator.Estimator object. This estimator will launch the training job.\n",
    "\n",
    "There are two kinds of parameters that need to be set for training.\n",
    "\n",
    "The first one are the parameters for the training job. These include: \n",
    "- (i) Training data path. This is S3 folder in which the input data is stored, \n",
    "- (ii) Output path: This the s3 folder in which the training output is stored. \n",
    "- (iii) Training instance type: This indicates the type of machine on which to run the training. Typically, we use GPU instances for these training. We defined the training instance type above to fetch the correct train_image_uri.\n",
    "\n",
    "The second set of parameters are algorithm specific training hyper-parameters.\n",
    "\n",
    "- Training instance count: This is the number of instances on which to run the training. When the number of instances is greater than one, then the image classification algorithm will run in distributed settings.\n",
    "- Training instance type: This indicates the type of machine on which to run the training. Typically, we use GPU instances for these training\n",
    "- Output path: This the s3 folder in which the training output is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Four channels: train, validation, train_lst, and validation_lst\n",
    "s3_train = f's3://{bucket}/{prefix}/train/'\n",
    "s3_validation = f's3://{bucket}/{prefix}/val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = f\"s3://{bucket}/{prefix}/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp model_images/upload_to_s3/train/ $s3_train --recursive --quiet\n",
    "!aws s3 cp model_images/upload_to_s3/val/ $s3_validation --recursive --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "For algorithm specific hyper-parameters, we start by fetching python dictionary of the training hyper-parameters that the algorithm accepts with their default values. This can then be overridden to custom values\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(model_id=model_id, model_version=model_version)\n",
    "\n",
    "# [Optional] Override default hyperparameters with custom values\n",
    "hyperparameters[\"epochs\"] = \"20\"\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with Automatic Model Tuning ([HPO](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html))\n",
    "\n",
    "---\n",
    "Amazon SageMaker automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose. We will use a [HyperparameterTuner](https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html) object to interact with Amazon SageMaker hyperparameter tuning APIs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter\n",
    "\n",
    "# Use AMT for tuning and selecting the best model\n",
    "use_amt = False\n",
    "\n",
    "# Define objective metric per framework, based on which the best model will be selected.\n",
    "metric_definitions_per_model = {\n",
    "    \"tensorflow\": {\n",
    "        \"metrics\": [\n",
    "            {\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"},\n",
    "            {\"Name\": \"val_top_5_accuracy\", \"Regex\": \"val_top_5_accuracy: ([0-9\\\\.]+)\"}            \n",
    "        ],\n",
    "        \"type\": \"Maximize\",\n",
    "    },\n",
    "    \"pytorch\": {\n",
    "        \"metrics\": [\n",
    "            {\"Name\": \"val_accuracy\", \"Regex\": \"val Acc: ([0-9\\\\.]+)\"},\n",
    "            {\"Name\": \"val_top_5_accuracy\", \"Regex\": \"val_top_5_accuracy: ([0-9\\\\.]+)\"}    \n",
    "        ],\n",
    "        \"type\": \"Maximize\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# You can select from the hyperparameters supported by the model, and configure ranges of values to be searched for training the optimal model.(https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html)\n",
    "hyperparameter_ranges = {\n",
    "    \"adam-learning-rate\": ContinuousParameter(0.0001, 0.1, scaling_type=\"Logarithmic\")\n",
    "}\n",
    "\n",
    "# Increase the total number of training jobs run by AMT, for increased accuracy (and training time).\n",
    "max_jobs = 6\n",
    "# Change parallel training jobs run by AMT to reduce total training time, constrained by your account limits.\n",
    "# if max_jobs=max_parallel_jobs then Bayesian search turns to Random.\n",
    "max_parallel_jobs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training\n",
    "---\n",
    "We start by creating the estimator object with all the required assets and then launch the training job (with spot instances).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use_spot_instances = False\n",
    "train_max_run = 1300\n",
    "train_max_wait = 2400 if train_use_spot_instances else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "training_job_name = name_from_base(f\"bc-detection-{model_id}-transfer-learning\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SageMaker Estimator instance\n",
    "ic_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    max_run=360000,\n",
    "    max_wait=640000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    use_spot_instances=True,\n",
    "    output_path=s3_output_location,\n",
    "    base_job_name=training_job_name,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_amt:\n",
    "    metric_definitions = next(\n",
    "        value for key, value in metric_definitions_per_model.items() if model_id.startswith(key)\n",
    "    )\n",
    "\n",
    "    hp_tuner = HyperparameterTuner(\n",
    "        ic_estimator,\n",
    "        metric_definitions[\"metrics\"][0][\"Name\"],\n",
    "        hyperparameter_ranges,\n",
    "        metric_definitions[\"metrics\"],\n",
    "        max_jobs=max_jobs,\n",
    "        max_parallel_jobs=max_parallel_jobs,\n",
    "        objective_type=metric_definitions[\"type\"],\n",
    "        base_tuning_job_name=training_job_name,\n",
    "    )\n",
    "\n",
    "    # Launch a SageMaker Tuning job to search for the best hyperparameters\n",
    "    hp_tuner.fit({\"training\": s3_train, \"validation\": s3_validation})\n",
    "else:\n",
    "    # Launch a SageMaker Training job by passing s3 path of the training data\n",
    "    ic_estimator.fit({\"training\": s3_train, \"validation\": s3_validation}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Assessment\n",
    "\n",
    "The model accuracy on the validation dataset is ~25% in predicting a specific class. On the other hand, the top_5_accuracy on the validation dataset is ~93%. Therefore, we can assume that our model is performs very well in guessing top five classes, which are likely indicators of breast cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy & run Inference on the fine-tuned model\n",
    "---\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means predicting the class label of an image. We follow the same steps as in 3. Run inference on the pre-trained model. We start by retrieving the jumpstart artifacts for deploying an endpoint. However, instead of base_predictor, we deploy the ic_estimator that we fine-tuned.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_instance_type = \"ml.m5.xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "# Retrieve the inference script uri\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-FT-{model_id}-\")\n",
    "\n",
    "# Use the estimator from the previous step to deploy to a SageMaker endpoint\n",
    "finetuned_predictor = (hp_tuner if use_amt else ic_estimator).deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    entry_point=\"inference.py\",\n",
    "    image_uri=deploy_image_uri,\n",
    "    source_dir=deploy_source_uri,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we query the fine-tuned model, parse the response and display the predictions.\n",
    "\n",
    "For this, we will make use of images excluded from the balanced dataset and select 10 images at random.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_metadata_df[~df_balanced]\n",
    "\n",
    "raw_df = train_metadata_df[~train_metadata_df.image_id.isin(df_balanced.image_id)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "model_engine = \"text-davinci-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(s):\n",
    "    \"\"\"\n",
    "    Function runs the prediction on an image\n",
    "    using the sagemaker inference endpoint\n",
    "    \"\"\"\n",
    "    dcm_file_path = ('train_images/' + s['patient_id'].astype(str) + '/' + s['image_id'].astype(str) + '.dcm').values[0]\n",
    "    png_img_object = convert_dcm_to_png_with_path(dcm_file_path)\n",
    "    buf = io.BytesIO()\n",
    "    png_img_object.save(buf, format='JPEG')\n",
    "    byte_im = buf.getvalue()\n",
    "    query_response = finetuned_predictor.predict(\n",
    "        byte_im, {\"ContentType\": \"application/x-image\", \"Accept\": \"application/json;verbose\"}\n",
    "    )\n",
    "    model_predictions = json.loads(query_response)\n",
    "    predicted_label = model_predictions[\"predicted_label\"]\n",
    "    return dict(zip(model_predictions['labels'], model_predictions['probabilities']))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_explanation(prediction_dict):\n",
    "    input_text = \"Explain the results of my cancer prediction model: \" + str(prediction_dict)\n",
    "    response = openai.Completion.create(\n",
    "        engine=model_engine,\n",
    "        prompt=input_text,\n",
    "        temperature=0.5,\n",
    "        max_tokens=1024,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        timeout=30,\n",
    "    )\n",
    "    explanation = response.choices[0].text.strip()\n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dict = show_predictions(raw_df.dropna().iloc[19:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = generate_explanation(prediction_dict)\n",
    "\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
